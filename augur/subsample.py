"""
Subsample sequences from an input dataset.

The input dataset can consist of a metadata file, a sequences file, or both.

See documentation page for details on configuration.
"""

from __future__ import annotations # can be removed once python 3.11 is our minimum
import argparse
import os
import subprocess
import sys
import tempfile
import threading
import yaml
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor, Future
from pathlib import Path
from textwrap import dedent
from typing import Any, Dict, List, Optional, Set, Tuple, Union
from augur import filter as augur_filter
from augur.argparse_ import ExtendOverwriteDefault, SKIP_AUTO_DEFAULT_IN_HELP
from augur.errors import AugurError
from augur.io.metadata import DEFAULT_DELIMITERS, DEFAULT_ID_COLUMNS
from augur.io.print import print_err, indented_list
from augur.utils import augur
from augur.validate import load_json_schema, validate_json, ValidateError
from augur.debug import DEBUGGING

BooleanFlags = Tuple[str, Optional[str]]
"""
Type for a boolean pair of augur filter command line flags that configure the
same option. When there is no second flag, absence of the first flag indicates
the default behavior.
"""

AugurFilterOption = Union[str, BooleanFlags]
"""
Type for an augur filter command line option. Either a single option or boolean
pair of flags.
"""

FilterArgs = Dict[str, Any]
"""
Augur filter arguments stored as a mapping from option to value.
"""

GLOBAL_CLI_OPTIONS: Dict[str, AugurFilterOption] = {
    "metadata": "--metadata",
    "metadata_chunk_size": "--metadata-chunk-size",
    "metadata_delimiters": "--metadata-delimiters",
    "metadata_id_columns": "--metadata-id-columns",
    "sequences": "--sequences",
    "sequence_index": "--sequence-index",
    "seed": "--subsample-seed",
}
"""
Mapping of argparse namespace variable name to augur filter option.
These are sent to both intermediate and final augur filter calls.
"""


FINAL_CLI_OPTIONS: Dict[str, AugurFilterOption] = {
    "output_metadata": "--output-metadata",
    "output_sequences": "--output-sequences",
    "output_log": "--output-log",
    "skip_checks": ("--skip-checks", None),
}
"""
Mapping of argparse namespace variable name to augur filter option.
These are sent to only the final augur filter call.
"""

# NOTE: If you edit any of these values, please re-run
# devel/regenerate-subsample-schema and commit the schema changes too.
SAMPLE_CONFIG: Dict[str, AugurFilterOption|None] = {
    "target_sample": None, # TODO XXX - config option which doesn't map directly to an augur filter cmd line arg
    "drop_sample": None, # TODO XXX - config option which doesn't map directly to an augur filter cmd line arg
    "exclude": "--exclude",
    "exclude_all": ("--exclude-all", None),
    "exclude_ambiguous_dates_by": "--exclude-ambiguous-dates-by",
    "exclude_where": "--exclude-where",
    "include": "--include",
    "include_where": "--include-where",
    "min_date": "--min-date",
    "max_date": "--max-date",
    "min_length": "--min-length",
    "max_length": "--max-length",
    "non_nucleotide": ("--non-nucleotide", None),
    "query": "--query",
    "query_columns": "--query-columns",
    "group_by": "--group-by",
    "group_by_weights": "--group-by-weights",
    "probabilistic_sampling": ("--probabilistic-sampling", "--no-probabilistic-sampling"),
    "sequences_per_group": "--sequences-per-group",
    "max_sequences": "--subsample-max-sequences",
}
"""
Mapping of YAML configuration key name to augur filter option.
These are sent to only the intermediate augur filter calls.
"""


def register_parser(parent_subparsers: argparse._SubParsersAction) -> argparse.ArgumentParser:
    parser = parent_subparsers.add_parser("subsample", help=__doc__)

    input_group = parser.add_argument_group("Input options", "options related to input files")
    input_group.add_argument('--metadata', metavar="FILE", help="sequence metadata" + SKIP_AUTO_DEFAULT_IN_HELP)
    input_group.add_argument('--sequences', metavar="FILE", help="sequences in FASTA or VCF format. For large inputs, consider using --sequence-index in addition to this option." + SKIP_AUTO_DEFAULT_IN_HELP)
    input_group.add_argument('--sequence-index', metavar="FILE", help="sequence composition report generated by augur index. If not provided, an index will be created on the fly. This should be generated from the same file as --sequences." + SKIP_AUTO_DEFAULT_IN_HELP)
    input_group.add_argument('--metadata-chunk-size', metavar="N", type=int, default=100000, help="maximum number of metadata records to read into memory at a time. Increasing this number can reduce run times at the cost of more memory used.")
    input_group.add_argument('--metadata-id-columns', metavar="COLUMN", default=DEFAULT_ID_COLUMNS, nargs="+", action=ExtendOverwriteDefault, help="names of possible metadata columns containing identifier information, ordered by priority. Only one ID column will be inferred.")
    input_group.add_argument('--metadata-delimiters', metavar="CHARACTER", default=DEFAULT_DELIMITERS, nargs="+", action=ExtendOverwriteDefault, help="delimiters to accept when reading a metadata file. Only one delimiter will be inferred.")
    input_group.add_argument('--skip-checks', action='store_true', help="use this option to skip checking for duplicates in sequences and whether ids in metadata have a sequence entry. Can improve performance on large files. Note that this should only be used if you are sure there are no duplicate sequences or mismatched ids since they can lead to errors in downstream Augur commands.")

    config_group = parser.add_argument_group("Configuration options", "options related to configuration")
    config_group.add_argument("--config", metavar="FILE", required=True, help="augur subsample config file. The expected config options must be defined at the top level, or within a specific section using --config-section." + SKIP_AUTO_DEFAULT_IN_HELP)
    config_group.add_argument("--config-section", metavar="KEY", nargs="+", action=ExtendOverwriteDefault, help="Use a section of the file given to --config by listing the keys leading to the section. Provide one or more keys. (default: use the entire file)" + SKIP_AUTO_DEFAULT_IN_HELP)
    config_group.add_argument("--search-paths", "--search-path", metavar="DIR", nargs="+", action=ExtendOverwriteDefault,
        help=dedent(f"""\
            One or more directories to search for relative filepaths specified
            in the config file. If a file exists in multiple directories, only
            the file from the first directory will be used. This can also be set
            via the environment variable 'AUGUR_SEARCH_PATHS'. Specified
            directories will be considered before the defaults, which are:
            (1) directory containing the config file
            (2) current working directory""" + SKIP_AUTO_DEFAULT_IN_HELP))
    config_group.add_argument('--nthreads', metavar="N", type=int, default=1, help="Number of CPUs/cores/threads/jobs to utilize at once. For augur subsample, this means the number of samples to run simultaneously. Individual samples are limited to a single thread. The final augur filter call can take advantage of multiple threads.")
    config_group.add_argument('--seed', metavar="N", type=int, help="random number generator seed for reproducible outputs (with same input data)." + SKIP_AUTO_DEFAULT_IN_HELP)

    output_group = parser.add_argument_group("Output options", "options related to output files")
    output_group.add_argument("--output-metadata", metavar="FILE", help="output metadata file" + SKIP_AUTO_DEFAULT_IN_HELP)
    output_group.add_argument("--output-sequences", metavar="FILE", help="output sequences file" + SKIP_AUTO_DEFAULT_IN_HELP)
    output_group.add_argument('--output-log', help="Tab-delimited file to debug sequence inclusion in samples. All sequences have a row with filter=filter_by_exclude_all. The sequences included in the output each have an additional row per sample that included it (there may be multiple). These rows have filter=force_include_strains with kwargs pointing to a temporary file that hints at the intermediate sample it came from." + SKIP_AUTO_DEFAULT_IN_HELP)
    return parser


def run(args: argparse.Namespace) -> None:
    """Run augur subsample.

    This is implemented by calling augur filter once for each sample in the
    config (i.e. the intermediate calls), then one more time to combine the
    samples (i.e. the final call). It was inspired by several pathogen repos
    adopting a similar approach using Snakemake rules.

    Notes on performance:

    - If multiple intermediate calls use sequence-based filters and
      --sequence-index is not set, each call will build its own sequence index,
      meaning the same work is done at least twice. A more optimal approach
      would be to add a preliminary step to build the sequence index then pass
      it down to the intermediate calls. However, this complicates things and
      may not be worth it if sequence indexing is rewritten:
      <https://github.com/nextstrain/augur/issues/1846>

    - If multiple intermediate calls use the same default filters that
      significantly reduce the size of the initial input dataset, each call will
      go through the large input dataset and filter it with the same filters,
      meaning the same work is done at least twice. A more optimal approach
      would be to run the default options through an initial augur filter call.
      This would output a much smaller intermediate dataset that can be used by
      the intermediate calls. However, this complicates things and may not be
      worth it if a proper input reuse approach such as database/parquet file
      support is adopted: <https://github.com/nextstrain/augur/issues/1574>
    """

    # Load schema, parse and validate config.
    schema_validator = load_json_schema("schema-subsample-config.json")
    config = _parse_config(args.config, args.config_section, schema_validator)

    # Resolve filepaths.
    search_paths = _get_search_paths(args.config, args.search_paths)
    config, _ = _resolve_filepaths(config, search_paths, schema_validator.schema)

    # Construct sample objects for augur filter calls
    # These contain the lists of arguments, but the arguments may be incomplete
    # since some samples depend on others. They will be completed when we create the DAG.

    defaults = config.get("defaults")
    samples: List[Sample] = []
    global_filter_args: FilterArgs = {}
    for cli_option, filter_option in GLOBAL_CLI_OPTIONS.items():
        if (value := getattr(args, cli_option)) is not None:
            _add_to_args(global_filter_args, filter_option, value)

    for name, options in config.get("samples", {}).items():
        options = _merge_options(options, defaults)
        
        # Is this sample an intermediate one and should be dropped from final output?
        drop = bool(options.pop('drop_sample', False))
        
        # Does this sample define a target sample, i.e. we need to use the outputs of
        # the target not the global input sequences
        target_sample = options.pop('target_sample', None)
        _global_filter_args = {**global_filter_args}
        if target_sample:
            _global_filter_args.pop("--metadata")  # key must exist
            _global_filter_args.pop("--sequences") # key must exist
            _global_filter_args.pop("--sequence_index", None) # key optional

        sample = Sample(name, options, _global_filter_args, drop=drop, target_sample=target_sample)
        samples.append(sample)

    final_filter_args: FilterArgs = {}
    for cli_option, filter_option in FINAL_CLI_OPTIONS.items():
        if (value := getattr(args, cli_option)) is not None:
            _add_to_args(final_filter_args, filter_option, value)

    # Resolve dependency ordering across samples
    # This wires up outputs→inputs for dependent samples
    dependents = _resolve_dag(samples)
    if DEBUGGING:
        _print_dag(samples, dependents)
    
    # Run augur filter.
    if len(samples) == 1:
        # A single sample is translated to a single augur filter call.
        # The list of ids (strains) is not needed in this case
        del samples[0].args["--output-strains"]
        samples[0].remove_temporary_files()
        filter_args = {
            **samples[0].args,
            **final_filter_args,
            "--nthreads": args.nthreads,
        }
        print(f"[{samples[0].name}] running as the only filter call necessary", flush=True)
        _run_final_filter(filter_args, samples[0].name)
    else:
        # Multiple samples require multiple augur filter calls.
        try:
            _run_samples(samples, dependents, args.nthreads)

            # Run the final augur filter call to combine the intermediate samples.
            # Exclude samples marked with drop=True (e.g. used only as upstream input).
            included_samples = [s for s in samples if not s.drop]
            final_filter_args.update(global_filter_args)
            final_filter_args.update({
                "--exclude-all": None,
                "--include": [s.outputs['strains'] for s in included_samples],
                "--nthreads": args.nthreads,
            })
            print(f"[collect samples] combining outputs from {len(included_samples)} samples", flush=True)
            _run_final_filter(final_filter_args, "collect samples")
        finally:
            for sample in samples:
                sample.remove_temporary_files()


def get_referenced_files(
    config_file: str,
    config_section: Optional[List[str]] = None,
    search_paths: Optional[List[str]] = None,
) -> Set[str]:
    """Get the files referenced in a subsample config file.

    Extracts and resolves all filepath values referenced in the config,
    including defaults and individual sample options.

    Parameters
    ----------
    config_file
        Path to the subsample config file.

    config_section
        Optional list of keys to navigate to a specific section of the config file.

    search_paths
        Optional list of directories to search for relative filepaths specified
        in the config file. If a file exists in multiple directories, only
        the file from the first directory will be used. This can also be set
        via the environment variable 'AUGUR_SEARCH_PATHS'. Specified
        directories will be considered before the defaults, which are:
        (1) directory containing the config file
        (2) current working directory

    Returns
    -------
    set
        Resolved filepaths
    """
    # Load schema, parse and validate config.
    schema_validator = load_json_schema("schema-subsample-config.json")
    config = _parse_config(config_file, config_section, schema_validator)

    # Resolve filepaths.
    search_path_objs = _get_search_paths(config_file, search_paths)
    config, filepaths = _resolve_filepaths(config, search_path_objs, schema_validator.schema)

    return set(filepaths)


def _parse_config(filename: str, config_section: Optional[List[str]], schema) -> Dict[str, Any]:
    # Create a custom YAML loader to treat timestamps as strings.
    class CustomLoader(yaml.SafeLoader):
        pass
    def string_constructor(loader, node):
        return loader.construct_scalar(node)
    CustomLoader.add_constructor('tag:yaml.org,2002:timestamp', string_constructor)

    with open(filename) as f:
        try:
            config = yaml.load(f, Loader=CustomLoader)
        except yaml.YAMLError as e:
            raise AugurError(f"The configuration file {filename!r} is not valid YAML.\n" + str(e)) from e

    # Handle --config-section.
    if config_section is not None:
        traversed_section = config

        for i, key in enumerate(config_section):
            if not isinstance(traversed_section, dict) or key not in traversed_section:
                traversed_path = ' → '.join(repr(key) for key in config_section[:i+1])
                raise AugurError(f"Config section {traversed_path} not found in {filename!r}")
            traversed_section = traversed_section[key]

        config = traversed_section

    # Validate against schema.
    try:
        validate_json(config, schema, filename)
    except ValidateError as e:
        raise AugurError(e)
    return config


def _get_search_paths(
    config_file: str,
    from_cli: List[str],
) -> List[Path]:
    """
    Returns the paths to search for relative filepaths in config.
    """
    default = [
        Path(config_file).parent,
        Path.cwd(),
    ]

    from_env = os.environ.get('AUGUR_SEARCH_PATHS')

    if from_cli:
        if from_env:
            print_err(dedent(f"""\
                WARNING: Both the command line argument --search-paths
                and the environment variable AUGUR_SEARCH_PATHS are set.
                Only the command line argument will be used."""))
        return [
            *(Path(p) for p in from_cli),
            *default,
        ]

    if from_env:
        return [
            *(Path(p) for p in from_env.split(':')),
            *default,
        ]

    return default


def _resolve_filepaths(
    config: Dict[str, Any],
    search_paths: List[Path],
    schema: Dict[str, Any],
    root_schema: Optional[Dict[str, Any]] = None,
) -> Tuple[Dict[str, Any], List[str]]:
    """
    Resolve filepaths in config.

    Recursively walks the config alongside the schema to determine which fields
    contain filepaths, resolves them, and collects the resolved filepaths.
    """
    if root_schema is None:
        root_schema = schema

    filepaths = []

    # Get properties schema for current section
    properties = schema.get("properties", {})
    pattern_properties = schema.get("patternProperties", {})

    for key, value in config.items():
        prop_schema = properties.get(key)

        if not prop_schema and pattern_properties:
            # Use first pattern property schema (for dynamic keys like samples)
            prop_schema = next(iter(pattern_properties.values()))

        # Get referenced property schema
        if ref := prop_schema.get("$ref"):
            prop_schema = _get_referenced_schema(ref, root_schema)

        # Resolve filepath
        if _is_filepath(prop_schema):
            if isinstance(value, list):
                config[key] = [str(_resolve_filepath(Path(v), search_paths)) for v in value]
                filepaths.extend(config[key])
            elif isinstance(value, str):
                config[key] = str(_resolve_filepath(Path(value), search_paths))
                filepaths.append(config[key])

        # Recurse into config section
        elif isinstance(value, dict):
            config[key], downstream_filepaths = _resolve_filepaths(
                value, search_paths, prop_schema, root_schema
            )
            filepaths.extend(downstream_filepaths)

    return config, filepaths


def _get_referenced_schema(
    ref: str,
    root_schema: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Resolve a JSON schema reference. Example: '#/$defs/sampleProperties'
    """
    keys = ref.lstrip("#/").split("/")
    schema = root_schema
    for key in keys:
        schema = schema[key]
    return schema


def _is_filepath(prop_schema: Dict[str, Any]) -> bool:
    """
    Check if the property schema declares it is a filepath.
    """
    # Direct 'format: filepath'
    if prop_schema.get("format") == "filepath":
        return True

    # Check oneOf variants for 'format: filepath'
    if "oneOf" in prop_schema:
        for variant in prop_schema["oneOf"]:
            if variant.get("format") == "filepath":
                return True

    return False


def _resolve_filepath(
    path: Path,
    search_paths: List[Path],
) -> Path:
    """
    Resolve a filepath by searching through multiple directories.

    If the path is already absolute, verify it exists and return it.

    >>> import tempfile
    >>> from pathlib import Path
    >>> tmpdir1 = Path(tempfile.mkdtemp()).resolve()
    >>> tmpdir2 = Path(tempfile.mkdtemp()).resolve()
    >>> absolute_path = tmpdir1 / "file.txt"
    >>> with open(absolute_path, "w") as f: _ = f.write("test")
    >>> _resolve_filepath(absolute_path, []) == absolute_path
    True

    Otherwise, try resolving it relative to each directory in search_paths, in order.
    Return the first path that exists.

    >>> with open(tmpdir2 / "file.txt", "w") as f: _ = f.write("test")
    >>> result = _resolve_filepath(Path("file.txt"), [tmpdir1, tmpdir2])
    >>> result == tmpdir1 / "file.txt"
    True

    If an absolute path doesn't exist, raise an error.

    >>> _resolve_filepath(Path("/nonexistent/file.txt"), [tmpdir1, tmpdir2])
    Traceback (most recent call last):
      ...
    augur.errors.AugurError: File '/nonexistent/file.txt' does not exist.

    If the relative path doesn't exist anywhere, raise an error.

    >>> _resolve_filepath(Path("nonexistent.txt"), [tmpdir1, tmpdir2]) # doctest: +ELLIPSIS
    Traceback (most recent call last):
      ...
    augur.errors.AugurError: File 'nonexistent.txt' not resolvable from any of the following paths:
    <BLANKLINE>
      ...
    """
    # Absolute path
    if path.is_absolute():
        if not path.exists():
            raise AugurError(f"File {str(path)!r} does not exist.")
        return path

    # Relative path
    for search_path in search_paths:
        resolved_path = (search_path / path).resolve()
        if resolved_path.exists():
            return resolved_path

    # File not found
    raise AugurError(dedent(f"""\
        File {str(path)!r} not resolvable from any of the following paths:

          {indented_list([str(p) for p in search_paths], '        ' + '  ')}"""))


def _merge_options(sample_options: Dict[str, Any], defaults: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Merge sample options with default options, with sample options taking precedence.
    """
    if defaults is None:
        return sample_options

    return {**defaults, **sample_options}


class Sample:
    """
    Construct 
    
    """
    def __init__(self, name: str, config: Dict[str, Any], global_filter_args: FilterArgs, /, drop: bool=False, target_sample: str|None=None) -> None:
        self.name = name
        self.augur_cmd = 'filter'
        self.outputs = {
            'strains': tempfile.NamedTemporaryFile(prefix=f"sample_{self.name}_", delete=False).name
        }
        self.drop = drop # flag indicating this sample shouldn't be used in the final output
        self.depends_on: dict[str, str] = {'target_sample': target_sample} if target_sample else {}
        self.incomplete:bool = bool(self.depends_on) # if it depends on other samples it will be incomplete until we run `use_outputs_from` for each
        self.args = self._construct_args(config, global_filter_args)
        
        if DEBUGGING:
            print("\n\nSample:", self.name, f"\n\t{self.drop=} {self.incomplete=} {self.depends_on=}\n\tArgs: ", self.args)

    def __repr__(self):
        # helps with debugging
        return f"Sample obj ({self.name})"

    def _construct_args(self, config: Dict[str, Any], global_filter_args: FilterArgs) -> FilterArgs:
        """
        Construct filter arguments from YAML config and global arguments.

        Extends global filter arguments:

        >>> global_args = {"--metadata": "test.tsv", "--sequences": "test.fasta"}
        >>> sample = Sample("test_sample", {}, global_args)
        >>> sample.args["--metadata"]
        'test.tsv'
        >>> sample.args["--sequences"]
        'test.fasta'

        Maps YAML config to filter arguments:

        >>> config = {"min_date": "2020-01-01", "group_by": ["region", "year"], "sequences_per_group": 5, "non_nucleotide": True, "probabilistic_sampling": False}
        >>> sample = Sample("test_sample", config, {})
        >>> sample.args["--min-date"]
        '2020-01-01'
        >>> sample.args["--group-by"]
        ['region', 'year']
        >>> sample.args["--sequences-per-group"]
        5
        >>> "--non-nucleotide" in sample.args
        True
        >>> "--no-probabilistic-sampling" in sample.args
        True
        """
        filter_args = {
            # Checks are redundant across multiple calls with the same input.
            # Checks will run once on the final augur filter call, unless explicitly skipped.
            "--skip-checks": None,

            # Use a single thread for each sample to simplify multithreading across samples.
            "--nthreads": 1,

            # Store outcome as a list of ids to be used by --include in the final augur filter call.
            "--output-strains": self.outputs['strains']
        }

        filter_args.update(global_filter_args)

        for config_key, value in config.items():
            filter_option = SAMPLE_CONFIG[config_key]
            _add_to_args(filter_args, filter_option, value)

        return filter_args

    def use_outputs_from(self, upstream: Sample) -> None:
        """
        Given an upstream Sample object, use its _outputs_ as _inputs_ for our sample (i.e. `self`).
        This requires modifying the upstream sample to output sequences & metadata which
        we can consume!

        When all upstream dependencies have been wired up, ``self.incomplete``
        is set to False automatically.
        """
        assert not upstream.incomplete, "use_outputs_from can only be run if the _upstream_ sample is complete"
        assert self.incomplete, "use_outputs_from can only be run if _this_ sample is incomplete"
        assert set(self.depends_on.keys()) == set(['target_sample'])
        assert upstream.name == self.depends_on['target_sample']

        # modify upstream to output more than just a strains list
        upstream.outputs['metadata']  = tempfile.NamedTemporaryFile(prefix=f"sample_{upstream.name}_metadata_", delete=False).name
        upstream.outputs['sequences'] = tempfile.NamedTemporaryFile(prefix=f"sample_{upstream.name}_sequences_", delete=False).name
        _add_to_args(upstream.args, "--output-metadata", upstream.outputs['metadata'])
        _add_to_args(upstream.args, "--output-sequences", upstream.outputs['sequences'])

        # and modify downstream (self!) to use these outputs as inputs
        _add_to_args(self.args, "--metadata", upstream.outputs['metadata'])
        _add_to_args(self.args, "--sequences", upstream.outputs['sequences'])

        self.incomplete = False

        if DEBUGGING:
            print(f"\n\nMapping dependencies between upstream sample {upstream.name} and downstream {self.name}")
            print("\n\tupstream:", upstream.args)
            print("\n\tdownstream:", self.args)

    def run(self) -> None:
        """Run augur filter as a subprocess.

        Notes:

        - A direct import of augur.filter in Python is not used because all
          samples would share the same sys.stderr, which causes interleaved
          messages when processes are run in parallel.
          This is also why _run_final_filter() isn't repurposed for use here.

        - shell=True is not used because it requires additional logic to
          carefully escape values such as "--metadata-delimiters , \t".
          This is also why run_shell_command() isn't used here.
        """
        assert not self.incomplete
        result = subprocess.run(
            [*augur(shell=False), self.augur_cmd, *_args_dict_to_list(self.args)],
            capture_output=True,
            text=True,
        )
        # Output console messages only after the command has completed so that
        # they are not mixed with messages from other samples running in
        # parallel.
        for line in result.stderr.strip().split('\n'):
            print_err(f"[{self.name}] {line}")

        if result.returncode != 0:
            error_msg = f"Sample {self.name!r} failed, see error above."
            raise AugurError(error_msg)

    def remove_temporary_files(self):
        """Remove any temporary outputs which may have been created"""
        for ftype, fname in self.outputs.items():
            if os.path.exists(fname):
                os.unlink(fname)
                if DEBUGGING:
                    print(f"Removed temporary file for sample {self.name} file type {ftype}")


def _run_samples(
    samples: List[Sample],
    dependents: Dict[str, List[str]],
    nthreads: int,
) -> None:
    """Run samples respecting dependency order using a single thread pool.

    Samples with no dependencies are submitted immediately. When a sample
    completes, any downstream samples whose dependencies are now fully
    satisfied are submitted. This allows maximum parallelism — a downstream
    sample starts as soon as its parent finishes, without waiting for an
    entire layer of unrelated samples.
    """
    by_name: Dict[str, Sample] = {s.name: s for s in samples}

    # Track how many unfinished parents each sample has
    # (Currently this maxes out at 1, but with forthcoming proximity subsampling it may be more)
    pending_deps: Dict[str, int] = {s.name: 0 for s in samples}
    for children in dependents.values():
        for child in children:
            pending_deps[child] += 1

    remaining = {s.name for s in samples}
    lock = threading.Lock()
    all_done = threading.Event()
    error: Optional[BaseException] = None
    futures: List[Future] = []

    def _on_complete(future: Future, name: str) -> None:
        """Callback fired when a sample's future completes."""
        nonlocal error

        # Cancelled futures (from our error handling) can be silently ignored
        if future.cancelled():
            return

        with lock:
            remaining.discard(name)

            exc = future.exception()
            if exc is not None:
                if error is None:
                    error = exc
                # Cancel any queued (not-yet-running) futures and don't
                # schedule new ones. Already-running samples will finish
                # naturally but no further work will be submitted.
                for f in futures:
                    f.cancel()
                remaining.clear()
                all_done.set()
                return

            # Don't schedule children if a previous sample already failed
            if error is not None:
                if len(remaining) == 0:
                    all_done.set()
                return

            # If this sample has dependents (i.e. we are the dependency of another sample)
            # Then decrease the running count of that samples dependencies
            # If the count is zero, then add it to the queue / thread pool
            for child in dependents.get(name, []):
                pending_deps[child] -= 1
                if pending_deps[child] == 0:
                    _submit(by_name[child])

            if len(remaining) == 0:
                all_done.set()

    def _submit(sample: Sample) -> None:
        """Submit a sample to the executor. Caller must hold lock."""
        f = executor.submit(sample.run)
        futures.append(f)
        f.add_done_callback(lambda fut, n=sample.name: _on_complete(fut, n)) # type: ignore[misc]

    executor = ThreadPoolExecutor(max_workers=nthreads)
    try:
        # Submit samples which have no dependencies
        with lock:
            for s in samples:
                if pending_deps[s.name] == 0:
                    _submit(s)

        # Wait for all samples (including dynamically submitted ones) to complete
        all_done.wait()
    finally:
        executor.shutdown(wait=True, cancel_futures=True)

    if error is not None:
        raise error


def _resolve_dag(samples: List[Sample]) -> Dict[str, List[str]]:
    """Validate sample dependencies and wire up outputs→inputs.

    Uses Kahn's algorithm to verify the dependency graph is a DAG (no cycles),
    then calls ``use_outputs_from`` to connect dependent samples.

    Parameters
    ----------
    samples
        List of Sample objects, some of which may have ``depends_on`` set.

    Returns
    -------
    dict
        Mapping of sample name → list of downstream sample names that depend
        on it. Used by the executor to schedule samples as their parents
        complete.

    Raises
    ------
    AugurError
        If a sample references a non-existent upstream sample, or if there is a
        dependency cycle.

    >>> # No dependencies → empty dependents
    >>> a = Sample.__new__(Sample); a.name = "a"; a.depends_on = {}; a.incomplete = False
    >>> b = Sample.__new__(Sample); b.name = "b"; b.depends_on = {}; b.incomplete = False
    >>> _resolve_dag([a, b])
    {}

    >>> # Simple dependency
    >>> a = Sample.__new__(Sample); a.name = "a"; a.depends_on = {}; a.incomplete = False
    >>> a.args = {}; a.outputs={}
    >>> b = Sample.__new__(Sample); b.name = "b"; b.depends_on = {"target_sample": "a"}; b.incomplete = True
    >>> b.args = {}; b.outputs={}
    >>> sorted(_resolve_dag([a, b]).items())  # doctest: +ELLIPSIS
    [('a', ['b'])]
    """
    samples_by_name: Dict[str, Sample] = {s.name:s for s in samples}

    # Validate references
    for s in samples:
        for dep_name in s.depends_on.values():
            if dep_name not in samples_by_name:
                raise AugurError(dedent(f"""\
                    Sample {s.name!r} depends on {dep_name!r} which is not a defined sample."""))

    # Build adjacency and in-degree for cycle detection
    in_degree: Dict[str, int] = {s.name: 0 for s in samples}
    dependents: Dict[str, List[str]] = defaultdict(list)
    for s in samples:
        for dep in s.depends_on.values():
            in_degree[s.name] += 1
            dependents[dep].append(s.name)

    # Kahn's algorithm for cycle detection and topological ordering
    queue = deque(name for name, deg in in_degree.items() if deg == 0)
    topo_order: List[str] = []
    while queue:
        name = queue.popleft()
        topo_order.append(name)
        for dep in dependents[name]:
            in_degree[dep] -= 1
            if in_degree[dep] == 0:
                queue.append(dep)

    if len(topo_order) != len(samples):
        raise AugurError("Dependency cycle detected among samples.")

    # Wire up outputs→inputs in topological order so that upstream samples
    # are always complete before their dependents are wired up.
    for name in topo_order:
        s = samples_by_name[name]
        if target_sample := s.depends_on.get('target_sample', None):
            s.use_outputs_from(samples_by_name[target_sample])

    # Return dictionary whose keys are samples which are dependencies for other samples
    # The values are a list of samples which depend on the (key) sample.
    # Note: samples which are not depended on by other samples will not be keys of this!
    return {k: v for k, v in dependents.items() if v}

def _print_dag(samples: List[Sample], dependents: Dict[str, List[str]]) -> None:
    """Print an ASCII representation of the sample dependency graph.

    Roots (no dependencies) are printed first, with their children indented
    below. Samples with no relationships are listed independently.

    >>> a = Sample.__new__(Sample); a.name = "a"; a.depends_on = {}; a.drop = False
    >>> b = Sample.__new__(Sample); b.name = "b"; b.depends_on = {"target_sample": "a"}; b.drop = False
    >>> c = Sample.__new__(Sample); c.name = "c"; c.depends_on = {}; c.drop = True
    >>> _print_dag([a, b, c], {"a": ["b"]})
    <BLANKLINE>
    Sample dependency graph:
      a
        └── b
      c (drop)
    <BLANKLINE>
    """
    has_parent = {child for children in dependents.values() for child in children}
    roots = [s for s in samples if s.name not in has_parent]

    lines = ["\nSample dependency graph:"]
    by_name = {s.name: s for s in samples}

    def _walk(name: str, prefix: str, connector: str) -> None:
        drop_tag = " (drop)" if by_name[name].drop else ""
        lines.append(f"{prefix}{connector}{name}{drop_tag}")
        children = dependents.get(name, [])
        child_prefix = prefix + ("    " if connector == "└── " else "│   " if connector == "├── " else "  ")
        for i, child in enumerate(children):
            _walk(child, child_prefix, "└── " if i == len(children) - 1 else "├── ")

    for root in roots:
        _walk(root.name, "  ", "")

    lines.append("")
    print("\n".join(lines))


def _add_to_args(args: FilterArgs, filter_option: AugurFilterOption, value: Any) -> None:
    """
    Add a filter option and its value to the arguments dictionary.

    Scalar values are added directly:

    >>> args = {}
    >>> _add_to_args(args, "--option", "value")
    >>> args
    {'--option': 'value'}

    List and tuple values are preserved:

    >>> args = {}
    >>> _add_to_args(args, "--option", ["a", "b", "c"])
    >>> args
    {'--option': ['a', 'b', 'c']}

    >>> args = {}
    >>> _add_to_args(args, "--option", ("a", "b"))
    >>> args
    {'--option': ('a', 'b')}

    Boolean true adds the true flag:

    >>> args = {}
    >>> _add_to_args(args, ("--enable", "--disable"), True)
    >>> args
    {'--enable': None}

    Boolean false adds the false flag if available:

    >>> args = {}
    >>> _add_to_args(args, ("--enable", "--disable"), False)
    >>> args
    {'--disable': None}

    Boolean false with no false flag adds nothing:

    >>> args = {}
    >>> _add_to_args(args, ("--enable", None), False)
    >>> args
    {}

    Multiple calls preserve all arguments:

    >>> args = {}
    >>> _add_to_args(args, "--option1", "a")
    >>> _add_to_args(args, "--option2", ["b", "c"])
    >>> _add_to_args(args, ("--option3", None), True)
    >>> args == {"--option1": "a", "--option2": ["b", "c"], "--option3": None}
    True
    """

    # Booleans are configured by one or two flags.
    if isinstance(value, bool):
        assert isinstance(filter_option, tuple) and len(filter_option) == 2
        true_flag, false_flag = filter_option
        if value is True:
            args[true_flag] = None
        else:
            assert value is False
            if false_flag is not None:
                args[false_flag] = None
            else:
                # No false flag implies a default value of false.
                pass
    else:
        # Everything else is configured by one option.
        assert isinstance(filter_option, str)

        # Store the value directly in the dictionary
        args[filter_option] = value


def _args_dict_to_list(args: FilterArgs) -> List[str]:
    """
    Convert an arguments dictionary to a list suitable for argparse and subprocesses.

    Options with no value are added as boolean flags:

    >>> _args_dict_to_list({"--option": None})
    ['--option']

    Scalar values are converted to strings and added with their option:

    >>> _args_dict_to_list({"--option": "value"})
    ['--option', 'value']

    >>> _args_dict_to_list({"--option": 5})
    ['--option', '5']

    List values are unpacked with the option followed by all values:

    >>> _args_dict_to_list({"--option": [1, 2, 3]})
    ['--option', '1', '2', '3']

    Tuple values are handled the same as lists:

    >>> _args_dict_to_list({"--option": (1, 2, 3)})
    ['--option', '1', '2', '3']

    Empty lists result in just the option name:

    >>> _args_dict_to_list({"--option": []})
    ['--option']
    """
    args_list: List[str] = []

    for filter_option, value in args.items():
        # No value means a boolean flag.
        if value is None:
            args_list.append(filter_option)

        # Lists and tuples are unpacked and given as strings.
        elif isinstance(value, (list, tuple)):
            args_list.extend([filter_option, *(str(v) for v in value)])

        # Everything else is scalar and given as strings.
        else:
            args_list.extend([filter_option, str(value)])

    return args_list


def _run_final_filter(filter_args: FilterArgs, name: str) -> None:
    """Run augur filter as part of the augur subsample process.

    Note: intermediate augur filter calls go through a separate subprocess to
    avoid interleaved messages during parallel execution - see Sample.run().
    This final call goes through the current Python process to take advantage of
    streamed messages, which we prepend with the ``name`` by using a custom stderr
    handler

    Parameters
    ----------
    filter_args
        Arguments for augur filter.
    name
        Label prepended to stderr output lines as ``[name]``.
    """
    parser = argparse.ArgumentParser()
    augur_filter.register_arguments(parser)
    args = parser.parse_args(_args_dict_to_list(filter_args))

    original_stderr = sys.stderr
    sys.stderr = _PrefixWriter(original_stderr, f"[{name}] ")
    try:
        augur_filter.run(args)
    finally:
        sys.stderr = original_stderr


class _PrefixWriter:
    """A file-like wrapper that prepends a prefix to each line written to the
    underlying stream."""

    def __init__(self, stream, prefix: str):
        self._stream = stream
        self._prefix = prefix
        self._at_line_start = True

    def write(self, text: str) -> int:
        if not text:
            return 0
        lines = text.split('\n')
        out = []
        for i, line in enumerate(lines):
            if i > 0:
                out.append('\n')
                self._at_line_start = True
            if line:
                if self._at_line_start:
                    out.append(self._prefix)
                out.append(line)
                self._at_line_start = False
        result = ''.join(out)
        self._stream.write(result)
        return len(text)

    def flush(self):
        self._stream.flush()

    def __getattr__(self, name):
        return getattr(self._stream, name)
